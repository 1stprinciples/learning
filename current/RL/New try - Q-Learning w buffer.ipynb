{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpy\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_convert(time):\n",
    "    time_max = 200\n",
    "    time_min = -200 \n",
    "    if time_min < time <= time_max:\n",
    "        min_max_time = (time-time_min)/(time_max-time_min)\n",
    "        scaled_time = min_max_time * 20 # Scaling time to be between 0 and 20\n",
    "        return int(np.ceil(scaled_time))\n",
    "    else:\n",
    "        raise Exception(\"Input time value outside range in def time_convert\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_convert(-199)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f(`machine.queue`) => OpenAI State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "\n",
    "import itertools \n",
    "\n",
    "import matplotlib \n",
    "\n",
    "import matplotlib.style \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import sys \n",
    "\n",
    "from collections import defaultdict \n",
    "import plotting \n",
    "import numpy as np\n",
    "  \n",
    "\n",
    "matplotlib.style.use('ggplot') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow shop environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.MultiDiscrete((3,3,3,3,21,21,21,21,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Tuple((gym.spaces.Discrete(3),gym.spaces.Discrete(3),\n",
    "                                                  gym.spaces.Discrete(3),gym.spaces.Discrete(3),\n",
    "                                                  gym.spaces.Discrete(21),gym.spaces.Discrete(21),\n",
    "                                                  gym.spaces.Discrete(21),gym.spaces.Discrete(21),\n",
    "                                                  gym.spaces.Discrete(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flowshopGym:\n",
    "    import random\n",
    "    # --------------------------------------INITIALIZE THINGS------------------------------------------\n",
    "    def __init__(self):\n",
    "        self.NUM_PRODUCTS = 20\n",
    "        self.IAT = 0.001 # change this to sample from some distribution\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.DEBUG = 1 # 1 = check simpy env ; 2 = check the openai processes\n",
    "        #self.observation_space = gym.spaces.Discrete(9)\n",
    "        # Rich observation space\n",
    "        # Product type for the 4 product type, time left for the 4 products, product type in machine\n",
    "        # For more details Thesis notes book on Notability\n",
    "        self.observation_space = gym.spaces.Tuple((gym.spaces.Discrete(3), # MC1 prod type\n",
    "                                                  gym.spaces.Discrete(3),gym.spaces.Discrete(3), # MC1 Queue prod type\n",
    "                                                  gym.spaces.Discrete(3),gym.spaces.Discrete(3),\n",
    "                                                  gym.spaces.Discrete(11),gym.spaces.Discrete(11), # MC1 Queue time left \n",
    "                                                  gym.spaces.Discrete(11),gym.spaces.Discrete(11)\n",
    "                                                  ))\n",
    "        #self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "        self.prod_and_requests = []\n",
    "        self.reward = 0 \n",
    "        self.cumulative_reward = 0 \n",
    "        self.machine1_prev_type = 0 # The previous product type in the machine\n",
    "        self.QUEUE_LENGTH = 4\n",
    "        \n",
    "        # The following two values from the setup time matrix\n",
    "        self.P1_to_P2 = 1\n",
    "        self.P2_to_P1 = 2 \n",
    "        \n",
    "        # The range for the time converter\n",
    "        self.TIME_MAX = 200\n",
    "        self.TIME_MIN = -200\n",
    "        \n",
    "        self.state_format = 'Hosp'\n",
    "    # --------------------------------------DEFINE PROCESSES------------------------------------------\n",
    "    \n",
    "    # Process 1 - Spawn of products\n",
    "    def process_1(self):\n",
    "        print(\"Entered new product process\")\n",
    "        \n",
    "        # Produce X number of products\n",
    "        for i in range(self.NUM_PRODUCTS):\n",
    "            \n",
    "            # Creating instances of the product \n",
    "            ###print(\"creating instance\", i)\n",
    "            self.instances.append(self.product(self,self.envSimpy,'Product_%d' % i, self.store, self.machine))\n",
    "            \n",
    "            # IAT TIMEOUT BEFORE THE NEXT SPAWN\n",
    "            #next_admission = random.expovariate(1 / interarrival_time)\n",
    "            yield self.envSimpy.timeout(self.IAT)\n",
    "        \n",
    "        # After all the products are put into the store, start process 2\n",
    "        # Process 2 - Getting products from the store and create machine request\n",
    "        self.envSimpy.process(self.process_2())\n",
    "    \n",
    "    class product(object):\n",
    "        def __init__(self, flowshopGym, envSimpy, name, number, machine):\n",
    "            \n",
    "            # Creating the required resources and environment\n",
    "            self.envSimpy = envSimpy\n",
    "\n",
    "            # State space variables\n",
    "            self.prod_type = random.randint(1,2) # the product type of the product.\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Rest of the variables\n",
    "            self.production_time = 2\n",
    "            self.production_end = random.randint(3,80)\n",
    "            self.name = name\n",
    "            self.number = number\n",
    "            self.machine = machine \n",
    "            \n",
    "            ###print(self.name,\"created\")\n",
    "            \n",
    "            # putting the product into the store\n",
    "            self.envSimpy.process(self.put_store())\n",
    "            \n",
    " \n",
    "\n",
    "        def put_store(self): # I am putting \n",
    "            yield sim.envSimpy.timeout(1)\n",
    "            ###print(\"Putting the\", self.name, \"in the store at\", self.envSimpy.now)\n",
    "            yield sim.store.put(self)\n",
    "            ###print(\"Items in the store\",len(sim.store.items),\" - time -\",self.envSimpy.now)\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    # -----------------------------PROCESS 2-------------------------------------------------  \n",
    "    def process_2(self): # If machine.queue is less than 4, get a prod from store and create request\n",
    "        while True:\n",
    "            if len(self.machine.queue) < self.QUEUE_LENGTH:\n",
    "                \n",
    "                # Get product from store\n",
    "                prod = yield self.store.get()\n",
    "                \n",
    "                # Request machine\n",
    "                ###print(\"requesting machine for\", prod.name)\n",
    "                req = prod.machine.request()\n",
    "\n",
    "                self.envSimpy.process(self.process_3(prod, req))\n",
    "            else:\n",
    "                yield self.envSimpy.timeout(1)\n",
    "\n",
    "    # -----------------------------PROCESS 3-------------------------------------------------  \n",
    "    def process_3(self,prod, req): # Wait for req to go through and then produce and release the machine  \n",
    "        sim.prod_and_requests.append([prod,req])\n",
    "        # Wait for the request to succeed\n",
    "        yield req\n",
    "        \n",
    "        \n",
    "        #Setup process if required\n",
    "        #print(\"setup time ### \",self.machine1_prev_type , prod.prod_type, \"at\", self.envSimpy.now )\n",
    "        if self.machine1_prev_type == 0:\n",
    "            ###print(\"Previous machine type was 0\")\n",
    "            pass # dont' do anything during the first pass\n",
    "        elif self.machine1_prev_type == prod.prod_type:\n",
    "            ###print(\"Same product type between MC and incoming product\")\n",
    "            pass # Dont' do anything if the product types are the same \n",
    "        elif prod.prod_type == 2: # current product type = 2 meaning we have to do a changeover from 1 to 2\n",
    "            yield self.envSimpy.timeout(self.P1_to_P2)\n",
    "            ###print(\"Setup time prod1 to prod2 finished at\",self.envSimpy.now )\n",
    "        elif prod.prod_type == 1: # current product type = 1 meaning we have to do a changeover from 2 to 1\n",
    "            yield self.envSimpy.timeout(self.P2_to_P1)\n",
    "            ###print(\"Setup time prod2 to prod1 finished at\",self.envSimpy.now )\n",
    "        else:\n",
    "            raise Exception(\"Sorry product type doesn't match available values [for Setup process]\")\n",
    "        #print(\"Time after setup time\", self.envSimpy.now)\n",
    "        \n",
    "        # ----------------------------\n",
    "        ###print('Start production of ' + str(prod.name) + ' at '+ str(self.envSimpy.now)  )\n",
    "\n",
    "        ###print(\"Production time\", prod.production_time)\n",
    "        yield self.envSimpy.timeout(prod.production_time)\n",
    "        ###print('Production finished at' + str(self.envSimpy.now))\n",
    "        # Changing the previous prod_type on the machine\n",
    "        self.machine1_prev_type = prod.prod_type\n",
    "        \n",
    "        prod.tardiness = self.envSimpy.now - prod.production_end   # -ve is good; +ve is bad \n",
    "        \n",
    "        self.info['Tardiness'] = prod.tardiness\n",
    "        # End of production \n",
    "        #print('End production of ' + str(prod.name) + ' at ' + str(self.envSimpy.now))\n",
    "\n",
    "\n",
    "        # Wait for an acceptance only when there are products in the machine.queue\n",
    "        if len(sim.machine.queue) != 0:\n",
    "            # 1. After production pass the time so that the agent can take action based on the latest state\n",
    "            # 2. Pass time when the action chosen by the agent is rejection\n",
    "            i = 0\n",
    "            while True:\n",
    "                if i == 0:\n",
    "                    yield self.envSimpy.timeout(1)\n",
    "                    print(\"Timeout after production\")\n",
    "                i += 1\n",
    "\n",
    "                \n",
    "                ###print(\"Initial queue\", sim.machine.queue)\n",
    "                ###print(\"Initial queue LENGTH\", len(sim.machine.queue))\n",
    "                ###print(\"Current_action\", sim.current_action)\n",
    "\n",
    "                if sim.current_action == 1: #rejected\n",
    "\n",
    "                    # rearrange the machine queue to reflect the rejection\n",
    "                    sim.machine.queue.insert(len(sim.machine.queue), sim.machine.queue.pop(0))\n",
    "                    ###print(\"Resultant queue\",sim.machine.queue)\n",
    "                    ###print(\"\\n\")\n",
    "                    #print(\"Action rejected; Passing time by 1 \", i)\n",
    "                    #print(\"Machine queue,prod types \", sim.machine.queue, sim.getObs())\n",
    "                    #print(\"Current time \" + str(self.envSimpy.now))\n",
    "                    #print(\"Machine product type \",)\n",
    "                    #print(\"Queue item 1 product type \", )\n",
    "                    yield self.envSimpy.timeout(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # rearrange the machine queue to reflect the rejection\n",
    "                    #self.machine.queue.insert(len(self.machine.queue), self.machine.queue.pop(0))\n",
    "\n",
    "\n",
    "                    # print the rearranged machine queue\n",
    "                    #print('#observation_after')\n",
    "                    #print(envGym.getObservation())\n",
    "\n",
    "\n",
    "\n",
    "                elif sim.current_action == 0: #if action accepted\n",
    "                    ###print(\"Resultant queue\",sim.machine.queue)\n",
    "                    ###print('\\n')\n",
    "                    break\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # Decide the reward \n",
    "        if prod.tardiness <= 0:\n",
    "            sim.reward = 10 \n",
    "        else: \n",
    "            sim.reward = 0\n",
    "\n",
    "        #print(\"Tardiness is \",prod.tardiness)\n",
    "        # Production has ended now I have to take the next action using take_action()\n",
    "        # envGym.machine.queue = envGym.take_action()\n",
    "        #envGym.machine.queue = []\n",
    "\n",
    "        # Release the machine\n",
    "        #yield prod.machine.release(req)\n",
    "        \n",
    "        # Release the machine\n",
    "        yield prod.machine.release(req)\n",
    "\n",
    "        # timeout before the next check\n",
    "        #yield env.timeout(1)\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # --------------------------------------DEFINE FUNCTIONS------------------------------------------\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def getObs(self): \n",
    "        # CREATING TUPLE/DICT TO HOLD INFO        \n",
    "        test_state = dict()\n",
    "        \n",
    "        # Machine 1 product type - Initial value\n",
    "        test_state['MC1_prod_type'] = 0\n",
    "        # Machine 1 queue product type - Initial values\n",
    "        test_state['MC1_queue1_type'] = 0\n",
    "        test_state['MC1_queue2_type'] = 0\n",
    "        test_state['MC1_queue3_type'] = 0\n",
    "        test_state['MC1_queue4_type'] = 0 \n",
    "        # Machine 1 queue time left - Initial values\n",
    "        test_state['MC1_queue1_timeleft'] = 0\n",
    "        test_state['MC1_queue2_timeleft'] = 0\n",
    "        test_state['MC1_queue3_timeleft'] = 0\n",
    "        test_state['MC1_queue4_timeleft'] = 0\n",
    "        \n",
    "        # 1. PRODUCT TYPE INSIDE THE MACHINE\n",
    "        if len(sim.machine.users) != 0: #if the machine is not empty\n",
    "            test_state['MC1_prod_type'] = (self.relater(sim.machine.users[0])).prod_type # Product type of the product inside machine 1\n",
    "        else:\n",
    "            pass # No product inside the machine\n",
    "        \n",
    "        \n",
    "        # 2. PRODUCT TYPE IN MACHINE 1 QUEUE\n",
    "        if len(sim.machine.queue) == 0: # queue is empty\n",
    "            pass # Already defined these values while creating the dict()\n",
    "\n",
    "        elif len(sim.machine.queue) == 1:\n",
    "            # Machine 1 queue1 product type\n",
    "            test_state['MC1_queue1_type'] = (self.relater(sim.machine.queue[0])).prod_type\n",
    "            \n",
    "        elif len(sim.machine.queue) == 2:\n",
    "            # Machine 1 queue1 product type\n",
    "            test_state['MC1_queue1_type'] = (self.relater(sim.machine.queue[0])).prod_type\n",
    "            \n",
    "            # Machine 1 queue2 product type\n",
    "            test_state['MC1_queue2_type'] = (self.relater(sim.machine.queue[1])).prod_type\n",
    "        \n",
    "        elif len(sim.machine.queue) == 3:\n",
    "            # Machine 1 queue1 product type\n",
    "            test_state['MC1_queue1_type'] = (self.relater(sim.machine.queue[0])).prod_type\n",
    "            \n",
    "            # Machine 1 queue2 product type\n",
    "            test_state['MC1_queue2_type'] = (self.relater(sim.machine.queue[1])).prod_type\n",
    "            \n",
    "            # Machine 1 queue3 product type\n",
    "            test_state['MC1_queue3_type'] = (self.relater(sim.machine.queue[2])).prod_type\n",
    "        \n",
    "        elif len(sim.machine.queue) == 4:\n",
    "            # Machine 1 queue1 product type\n",
    "            test_state['MC1_queue1_type'] = (self.relater(sim.machine.queue[0])).prod_type\n",
    "            \n",
    "            # Machine 1 queue2 product type\n",
    "            test_state['MC1_queue2_type'] = (self.relater(sim.machine.queue[1])).prod_type\n",
    "            \n",
    "            # Machine 1 queue3 product type\n",
    "            test_state['MC1_queue3_type'] = (self.relater(sim.machine.queue[2])).prod_type\n",
    "            \n",
    "            # Machine 1 queue4 product type\n",
    "            test_state['MC1_queue4_type'] = (self.relater(sim.machine.queue[3])).prod_type\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Queue length incompatible\")\n",
    "\n",
    "        \n",
    "        # 3. TIME LEFT CALCULATION\n",
    "        # for each position\n",
    "        if test_state['MC1_queue1_type'] != 0: # product present\n",
    "            time_left = self.time_left((self.relater(sim.machine.queue[0])).production_end)\n",
    "            test_state['MC1_queue1_timeleft'] = self.time_convert(time_left)\n",
    "                \n",
    "        if test_state['MC1_queue2_type'] != 0: # product present\n",
    "            time_left = self.time_left((self.relater(sim.machine.queue[1])).production_end)\n",
    "            test_state['MC1_queue2_timeleft'] = self.time_convert(time_left)\n",
    "        \n",
    "        if test_state['MC1_queue3_type'] != 0: # product present\n",
    "            time_left = self.time_left((self.relater(sim.machine.queue[2])).production_end)\n",
    "            test_state['MC1_queue3_timeleft'] = self.time_convert(time_left)\n",
    "        \n",
    "        if test_state['MC1_queue4_type'] != 0: # product present\n",
    "            time_left = self.time_left((self.relater(sim.machine.queue[3])).production_end)\n",
    "            test_state['MC1_queue4_timeleft'] = self.time_convert(time_left)\n",
    "            \n",
    "        obs = [v for k,v in test_state.items()]    \n",
    "        #print(\"------ obs \", tuple(obs))\n",
    "        \n",
    "        return tuple(obs)\n",
    "    \n",
    "    def time_left(self, due):\n",
    "        return (due - sim.envSimpy.now)\n",
    "    \n",
    "    def time_convert(self, time):\n",
    "        time_max = self.TIME_MAX\n",
    "        time_min = self.TIME_MIN\n",
    "        \n",
    "        if time_min < time <= time_max:\n",
    "            min_max_time = (time-time_min)/(time_max-time_min)\n",
    "            scaled_time = min_max_time * 10 # Scaling time to be between 0 and 20\n",
    "            return int(np.ceil(scaled_time))\n",
    "        else:\n",
    "            raise Exception(\"Input time value outside range in def time_convert\")\n",
    "    \n",
    "     \n",
    "    def encode(self, obs):\n",
    "        print('Entered encode')\n",
    "        # (3), 3 [Machine queue type, machine product type]\n",
    "        i = obs[0] \n",
    "        i *= 3\n",
    "        \n",
    "        i += obs[1]\n",
    "        #print(\"Value of i inside encode \", i)\n",
    "        return i\n",
    "    \n",
    "    def doneFn(self):\n",
    "        # So the condition is, \n",
    "        #(no items in queue1 + queue2 + ...) AND \n",
    "        #(no machine is running)\n",
    "        if self.machine.queue != None:\n",
    "            lenQueue = len(self.machine.queue)\n",
    "        else: \n",
    "            lenQueue = 0 \n",
    "        lenMachinesUsed = self.machine.count\n",
    "        #print(\"Queue length\",lenQueue, \"no machines used\", lenMachinesUsed)\n",
    "        if lenQueue == 0 and lenMachinesUsed == 0: \n",
    "            #print(\"length of queue \",lenQueue,\"Machines used \", lenMachinesUsed)\n",
    "            return True\n",
    "        else:\n",
    "            #print(\"length of queue \",lenQueue,\"Machines used \", lenMachinesUsed)\n",
    "            return False\n",
    "    \n",
    "    # A function to relate products and requests\n",
    "    def relater(self,item):\n",
    "        # Note that the input to this function shouldn't be a list. It should be of \n",
    "        # type request or product\n",
    "        #print(\"Entered relater with this item ###\", item, type(item))\n",
    "        output_item = None\n",
    "        \n",
    "        if str(type(item)) != \"<class '__main__.flowshopGym.product'>\" and str(type(item)) != 'simpy.resources.resource.Request':\n",
    "            raise Exception(\"Passed in a list to relater. Expecting request or product object\")\n",
    "            #print(\"Error passed in a list. Expecting request or product object\")\n",
    "            return None\n",
    "        # If the input is a request\n",
    "        #print(\"self.prod_and_requests\", self.prod_and_requests)\n",
    "        if type(item) == simpy.resources.resource.Request:\n",
    "            for i, j in enumerate(self.prod_and_requests):\n",
    "                if j[1] == item:\n",
    "                    output_item = j[0]\n",
    "                    return output_item\n",
    "        else: #if the input is a product \n",
    "            for i, j in enumerate(self.prod_and_requests):\n",
    "                if j[0] == item:\n",
    "                    output_item = j[1]\n",
    "                    return output_item   \n",
    "    \n",
    "    \n",
    "    # --------------------------------------RENDER------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # --------------------------------------RESET------------------------------------------\n",
    "    def reset(self):\n",
    "        print(\"Entered Reset\")\n",
    "        # Initialise simpy environemnt and a machine\n",
    "        self.envSimpy = simpy.Environment()\n",
    "        self.machine = simpy.Resource(self.envSimpy,capacity = 1)\n",
    "        self.store = simpy.Store(self.envSimpy) \n",
    "        \n",
    "        # Initial variable definitions\n",
    "        self.time_start = self.envSimpy.now\n",
    "        self.next_time_stop = self.time_start + 1\n",
    "        self.time_step = 1\n",
    "        self.time_step_terminal = self.time_start + 100\n",
    "        self.current_action = 0\n",
    "        self.instances = []\n",
    "         \n",
    "        # Set up starting processes\n",
    "        self.envSimpy.process(self.process_1())\n",
    "        \n",
    "        # Set starting state values\n",
    "\n",
    "        \n",
    "        # Inital load of patients (to average occupancy)\n",
    "        #self._load_patients()\n",
    "        \n",
    "        # Starting values of observations\n",
    "        observations = self.getObs()\n",
    "        \n",
    "        # Put state dictionary items into observations list (Define observations)\n",
    "        #observations = [v for k,v in self.state.items()]\n",
    "        return observations\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # --------------------------------------STEP------------------------------------------\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        # Define params\n",
    "        self.current_action = action\n",
    "        self.action_taken = False \n",
    "        self.info = {}\n",
    "        self.reward = 0 \n",
    "        #print(\"machine is used by\",sim.machine.users)\n",
    "        # Run for time_step\n",
    "        self.next_time_stop += self.time_step\n",
    "        self.envSimpy.run(until = self.next_time_stop)\n",
    "    \n",
    "        # Get values\n",
    "        observation = self.getObs()\n",
    "        done = self.doneFn()\n",
    "        info = self.info\n",
    "        self.cumulative_reward += self.reward\n",
    "        #print(\"Values from Step\",observation, sim.reward, done, info)\n",
    "        # Return values\n",
    "        return (observation, sim.reward, done, info)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = flowshopGym()\n",
    "state = sim.reset()\n",
    "epochs = 0\n",
    "penalties, reward, epoch_reward = 0, 0, 0 \n",
    "epsilon = 0.1\n",
    "done = False\n",
    "tardiness = []\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Random action\n",
    "    action = sim.action_space.sample()\n",
    "    \n",
    "    # FIFO\n",
    "    #action = 0\n",
    "    \n",
    "    \n",
    "    #action = np.argmax(Q1[state])\n",
    "    #if random.uniform(0, 1) < epsilon:\n",
    "    #    action = sim.action_space.sample() # Explore action space\n",
    "    #else:\n",
    "    #    action = np.argmax(Q1[state]) # Exploit learned values\n",
    "    ###print(\"###Taking an action at\", sim.envSimpy.now)\n",
    "    state, reward, done, info = sim.step(action)\n",
    "    if reward == 0:\n",
    "        penalties += 1\n",
    "    try:\n",
    "        tardiness.append(info['Tardiness'])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    epoch_reward += reward\n",
    "    \n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "    \n",
    "mean_tardiness = sum(tardiness)/(len(tardiness))\n",
    "print(\"End time is \", sim.envSimpy.now)\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))\n",
    "print(\"Epoch reward \", epoch_reward)\n",
    "print(\"Mean tardiness of epoch \", mean_tardiness, \"(-ve is good)\")\n",
    "#print(\"Median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tardiness +ve is bad and -ve is good\n",
    "plt.plot(tardiness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tardiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tardiness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall flow - Mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim = flowshopGym()\n",
    "sim.reset()\n",
    "while True: # This while is for one full cycle of products (20 nos, currently)\n",
    "\n",
    "    # Choosing the action\n",
    "    #action = envGym.action_space.sample() # Random agent\n",
    "    # action = 0 #FIFO\n",
    "    #action = random.randint(0,1)\n",
    "    action = sim.action_space.sample()\n",
    "    print('Action space from the gym ', action)\n",
    "    # Taking a step\n",
    "    print(\"---------------New Cycle----------------\")\n",
    "    #print(\"Action from the agent \" + str(action))\n",
    "    obs, reward, done, info = sim.step(action) # the env here is gym\n",
    "    if done:\n",
    "        print( \"End of simulation at \" ,sim.envSimpy.now)\n",
    "        print(\"Total cumulative reward \", sim.cumulative_reward)\n",
    "        break\n",
    "    print(\"Reward achieved is \", reward)\n",
    "\n",
    "    #print(\"Action chosen by the agent \", action)\n",
    "    #print(\"Action taken by the environment \", )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.floor(-41.72999999999972)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall flow - Q-learning - Taxi env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#q_table = np.zeros([sim.observation_space.n, sim.action_space.n])\n",
    "Q1 = defaultdict(lambda: np.zeros(sim.action_space.n)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sim = flowshopGym()\n",
    "#sim.reset()\n",
    "\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "all_rewards = []\n",
    "all_tardiness = []\n",
    "\n",
    "for i in range(1, 101):\n",
    "    state = sim.reset()\n",
    "\n",
    "    epochs, penalties, reward, episodic_reward, tardiness = 0, 0, 0, 0, []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = sim.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(Q1[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = sim.step(action) \n",
    "        ###print(\"state, action\", state, action)\n",
    "        old_value = Q1[state][action]\n",
    "        next_max = np.max(Q1[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        Q1[state][action] = new_value\n",
    "        \n",
    "        episodic_reward += reward\n",
    "        \n",
    "        if reward == 0:\n",
    "            penalties += 1\n",
    "        \n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "        try:\n",
    "            print(info['Tardiness'])\n",
    "            tardiness.append(info['Tardiness'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    all_epochs.append(i)\n",
    "    all_penalties.append(penalties)\n",
    "    all_rewards.append(episodic_reward)\n",
    "    all_tardiness.append(np.mean(tardiness))\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_epochs,all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_epochs,all_penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_epochs,all_tardiness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(all_tardiness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_penalties,all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q - learning implementation 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions): \n",
    "\n",
    "    \"\"\" \n",
    "\n",
    "    Creates an epsilon-greedy policy based \n",
    "\n",
    "    on a given Q-function and epsilon. \n",
    "\n",
    "       \n",
    "\n",
    "    Returns a function that takes the state \n",
    "a\n",
    "    as an input and returns the probabilities \n",
    "\n",
    "    for each action in the form of a numpy array  \n",
    "\n",
    "    of length of the action space(set of possible actions). \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def policyFunction(state):\n",
    "        \n",
    "\n",
    "        Action_probabilities = np.ones(num_actions, \n",
    "\n",
    "                dtype = float) * epsilon / num_actions \n",
    "\n",
    "                  \n",
    "\n",
    "        best_action = np.argmax(Q[state]) \n",
    "\n",
    "        Action_probabilities[best_action] += (1.0 - epsilon) \n",
    "\n",
    "        return Action_probabilities \n",
    "\n",
    "   \n",
    "\n",
    "    return policyFunction \n",
    " \n",
    "\n",
    "def qLearning(env, num_episodes, discount_factor = 1.0, \n",
    "\n",
    "                            alpha = 0.6, epsilon = 0.1): \n",
    "\n",
    "    \"\"\" \n",
    "\n",
    "    Q-Learning algorithm: Off-policy TD control. \n",
    "\n",
    "    Finds the optimal greedy policy while improving \n",
    "\n",
    "    following an epsilon-greedy policy\"\"\"\n",
    "\n",
    "       \n",
    "\n",
    "    # Action value function \n",
    "\n",
    "    # A nested dictionary that maps \n",
    "\n",
    "    # state -> (action -> action-value). \n",
    "\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n)) \n",
    "\n",
    "   \n",
    "\n",
    "    # Keeps track of useful statistics \n",
    "\n",
    "    stats = plotting.EpisodeStats( \n",
    "\n",
    "        episode_lengths = np.zeros(num_episodes), \n",
    "\n",
    "        episode_rewards = np.zeros(num_episodes))     \n",
    "\n",
    "       \n",
    "\n",
    "    # Create an epsilon greedy policy function \n",
    "\n",
    "    # appropriately for environment action space \n",
    "\n",
    "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n) \n",
    "\n",
    "       \n",
    "\n",
    "    # For every episode \n",
    "\n",
    "    for ith_episode in range(num_episodes): \n",
    "\n",
    "           \n",
    "\n",
    "        # Reset the environment and pick the first action \n",
    "\n",
    "        state = env.reset() \n",
    "        #print(\"This is my state format \",state)\n",
    "           \n",
    "\n",
    "        for t in itertools.count(): \n",
    "\n",
    "               \n",
    "\n",
    "            # get probabilities of all actions from current state \n",
    "\n",
    "            action_probabilities = policy(state) \n",
    "\n",
    "   \n",
    "\n",
    "            # choose action according to  \n",
    "\n",
    "            # the probability distribution \n",
    "\n",
    "            action = np.random.choice(np.arange( \n",
    "\n",
    "                      len(action_probabilities)), \n",
    "\n",
    "                       p = action_probabilities) \n",
    "\n",
    "   \n",
    "\n",
    "            # take action and get reward, transit to next state \n",
    "\n",
    "            next_state, reward, done, _ = env.step(action) \n",
    "            print(\"This is the next state from step \",next_state)\n",
    "               \n",
    "\n",
    "            # Update statistics \n",
    "\n",
    "            stats.episode_rewards[ith_episode] += reward \n",
    "\n",
    "            stats.episode_lengths[ith_episode] = t \n",
    "\n",
    "               \n",
    "\n",
    "            # TD Update \n",
    "\n",
    "            best_next_action = np.argmax(Q[next_state])     \n",
    "\n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action] \n",
    "\n",
    "            td_delta = td_target - Q[state][action] \n",
    "\n",
    "            Q[state][action] += alpha * td_delta \n",
    "\n",
    "   \n",
    "\n",
    "            # done is True if episode terminated    \n",
    "\n",
    "            if done: \n",
    "\n",
    "                break\n",
    "\n",
    "                   \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "       \n",
    "\n",
    "    return Q, stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = flowshopGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Q, stats = qLearning(sim, 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_episode_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(Q[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = defaultdict(lambda: np.zeros(sim.action_space.n)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Q1:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(sim.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "205px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
